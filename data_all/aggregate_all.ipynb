{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statistics\n",
    "import warnings\n",
    "from scipy.stats import pearsonr\n",
    "warnings.filterwarnings('ignore')\n",
    "from itertools import combinations\n",
    "import random\n",
    "from seqeval.metrics import f1_score,classification_report,accuracy_score\n",
    "from sklearn.metrics import f1_score as bi_f1_score\n",
    "from sklearn.metrics import accuracy_score as bi_accuracy_score\n",
    "from sklearn.metrics import classification_report as bi_classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import traceback\n",
    "import html #for unescape & < >\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import emoji\n",
    "import os\n",
    "from nervaluate import Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 842,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_200=pd.read_json('filtered_initial_batch.json')\n",
    "data_3000=pd.read_csv('filtered_main_batches.json')\n",
    "# data=pd.read_csv('/Users/ruofanhu/Desktop/Projects/USDA/Mturk/200_results_drop1.csv')\n",
    "# data=pd.concat([data_200,d_3000])\n",
    "dj_3000=data_3000.to_json(orient=\"records\")\n",
    "parsed_3000 = json.loads(dj_3000)\n",
    "\n",
    "dj_200=data_200.to_json(orient=\"records\")\n",
    "parsed_200 = json.loads(dj_200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 843,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_entity_label_200(record):\n",
    "    txt = html.unescape(record['Input.tweet'])\n",
    "    \n",
    "    temp = list(filter(None, re.split('([,.!?:()[\\]\\\\/\"“”\\s+])', txt)))\n",
    "\n",
    "    # remove space strings from list and convert into np array\n",
    "    tweet_split = np.array(list(filter(str.strip, temp)))\n",
    "\n",
    "\n",
    "    token_labels = np.array(['O']*len(tweet_split),dtype=np.dtype(('U',10)))\n",
    "#     if record['Answer.related_index'] != '[]' :\n",
    "#         relation_lables_idx_str = sum([i.split(' ') for i in ast.literal_eval(record['Answer.related_index'])],[])\n",
    "\n",
    "#         relation_lables_idx = list(map(int, relation_lables_idx_str))\n",
    "#         relation_lables[relation_lables_idx] = 1\n",
    "\n",
    "                \n",
    "    if record['Answer.no-entity'] is None and re.split('[|]', record[\"Answer.html_output\"])[1]!='': # the value is 1 when there is no entity to label\n",
    "        html_output_list = ast.literal_eval(re.split('[|]', record[\"Answer.html_output\"])[1])\n",
    "                \n",
    "        for e in html_output_list:\n",
    "            if 'idx' in list(e.keys()):\n",
    "\n",
    "                if ' ' in e['idx']:\n",
    "                    idx = list(map(int, e['idx'].split(' ')))\n",
    "                else:\n",
    "                    idx = ast.literal_eval(e['idx'])\n",
    "\n",
    "                if type(idx) is int:\n",
    "\n",
    "#                             assert tweet_split[idx] == e['text']\n",
    "                    token_labels[idx] = 'B-'+e['className'].split('-')[1]\n",
    "                else:\n",
    "                    idx=list(idx)\n",
    "c\n",
    "    return token_labels.tolist()\n",
    "\n",
    "def process_entity_label_3000(record):\n",
    "    txt = html.unescape(record['Input.tweet'])\n",
    "    \n",
    "    temp = list(filter(None, re.split('([,.!?:()[\\]\"\\s+])', txt)))\n",
    "\n",
    "    # remove space strings from list and convert into np array\n",
    "    tweet_split = np.array(list(filter(str.strip, temp)))\n",
    "\n",
    "\n",
    "    token_labels = np.array(['O']*len(tweet_split),dtype=np.dtype(('U',10)))\n",
    "#     if record['Answer.related_index'] != '[]' :\n",
    "#         relation_lables_idx_str = sum([i.split(' ') for i in ast.literal_eval(record['Answer.related_index'])],[])\n",
    "\n",
    "#         relation_lables_idx = list(map(int, relation_lables_idx_str))\n",
    "#         relation_lables[relation_lables_idx] = 1\n",
    "\n",
    "                \n",
    "    if record['Answer.no-entity'] is None and re.split('[|]', record[\"Answer.html_output\"])[1]!='': # the value is 1 when there is no entity to label\n",
    "        html_output_list = ast.literal_eval(re.split('[|]', record[\"Answer.html_output\"])[1])\n",
    "                \n",
    "        for e in html_output_list:\n",
    "            if 'idx' in list(e.keys()):\n",
    "\n",
    "                if ' ' in e['idx']:\n",
    "                    idx = list(map(int, e['idx'].split(' ')))\n",
    "                else:\n",
    "                    idx = ast.literal_eval(e['idx'])\n",
    "\n",
    "                if type(idx) is int:\n",
    "\n",
    "#                             assert tweet_split[idx] == e['text']\n",
    "                    token_labels[idx] = 'B-'+e['className'].split('-')[1]\n",
    "                else:\n",
    "                    idx=list(idx)\n",
    "                    token_labels[idx[0]] = 'B-'+e['className'].split('-')[1]\n",
    "                    token_labels[idx[1:]] = 'I-' + e['className'].split('-')[1]\n",
    "\n",
    "    return token_labels.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 844,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewsPerUser = defaultdict(list)\n",
    "reviewsPerItem = defaultdict(list)\n",
    "reviewsPertweet = defaultdict(list)\n",
    "for d in parsed_3000:\n",
    "    entity_labels=process_entity_label_3000(d)\n",
    "    reviewsPertweet[d['InputId']].append([1]+[0]*(len(entity_labels)-1))\n",
    "\n",
    "\n",
    "    for i in range(len(entity_labels)):\n",
    "        user,item = d['WorkerId'], str(d['InputId'])+','+str(i)\n",
    "        reviewsPerUser[user].append(d)\n",
    "        reviewsPerItem[item].append({'WorkerId':d['WorkerId'],'entity_type':entity_labels[i]})\n",
    "\n",
    "for d in parsed_200:\n",
    "    entity_labels=process_entity_label_200(d)\n",
    "    reviewsPertweet[d['InputId']].append([1]+[0]*(len(entity_labels)-1))\n",
    "\n",
    "\n",
    "    for i in range(len(entity_labels)):\n",
    "        user,item = d['WorkerId'], str(d['InputId'])+','+str(i)\n",
    "        reviewsPerUser[user].append(d)\n",
    "        reviewsPerItem[item].append({'WorkerId':d['WorkerId'],'entity_type':entity_labels[i]})\n",
    "\n",
    "        \n",
    "lu = len(reviewsPerUser)\n",
    "li = len(reviewsPerItem)\n",
    "data_ = np.empty((li, lu))\n",
    "data_[:] = np.nan\n",
    "# data__t = np.empty((lu, li))\n",
    "ku = list(reviewsPerUser.keys())\n",
    "ki = list(reviewsPerItem.keys())\n",
    "\n",
    "# for i in range(li):\n",
    "#     for r in reviewsPerItem[ki[i]]:\n",
    "#         data_[i][ku.index(r['WorkerId'])] =r['entity_type']\n",
    "\n",
    "# Construct the P Matrix\n",
    "data_m = pd.DataFrame('',columns=ku,index=ki)\n",
    "for i in range(li):\n",
    "    for r in reviewsPerItem[ki[i]]:\n",
    "        data_m.loc[ki[i]][ku.index(r['WorkerId'])] =r['entity_type']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 845,
   "metadata": {},
   "outputs": [],
   "source": [
    "list1=[i.split(',')[0] for i in list(data_m.index)]\n",
    "all_tweet_id = []\n",
    " \n",
    "for i in list1:\n",
    "    if i not in all_tweet_id:\n",
    "        all_tweet_id.append(i)\n",
    "tokens_count=[list1.count(i) for i in all_tweet_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2num={'O':8,'':-1,'B-food':2, 'B-loc':0, 'B-other':6, 'B-symptom':4,'I-food':3, 'I-loc':1,\n",
    "       'I-other':7, 'I-symptom':5}\n",
    "\n",
    "num2label={v:k for k, v in label2num.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{8: 'O',\n",
       " -1: '',\n",
       " 2: 'B-food',\n",
       " 0: 'B-loc',\n",
       " 6: 'B-other',\n",
       " 4: 'B-symptom',\n",
       " 3: 'I-food',\n",
       " 1: 'I-loc',\n",
       " 7: 'I-other',\n",
       " 5: 'I-symptom'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [],
   "source": [
    "#annotations\n",
    "new_data_m=data_m.replace(label2num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create doc_start and features\n",
    "ppdict = {n: grp.loc[n].to_dict('index') for n, grp in data_3000.set_index(['InputId', 'WorkerId']).groupby(level='Input.id')}\n",
    "\n",
    "doc_start=[]\n",
    "features=[]\n",
    "for tweet_id,collection in ppdict.items():\n",
    "    basic_info=list(collection.values())[0]\n",
    "    txt = html.unescape(basic_info['Input.tweet'])\n",
    "    temp = list(filter(None, re.split('([,.!?:()[\\]\"\\s+])', txt)))\n",
    "\n",
    "    # remove space strings from list and convert into np array\n",
    "    tweet_split = list(filter(str.strip, temp))\n",
    "    length=len(tweet_split)\n",
    "    doc_start.extend([1]+[0]*(length-1))\n",
    "    features.extend(tweet_split)\n",
    "\n",
    "ppdict = {n: grp.loc[n].to_dict('index') for n, grp in data_200.set_index(['InputId', 'WorkerId']).groupby(level='Input.id')}\n",
    "\n",
    "for tweet_id,collection in ppdict.items():\n",
    "    basic_info=list(collection.values())[0]\n",
    "    txt = html.unescape(basic_info['Input.tweet'])\n",
    "    temp = list(filter(None, re.split('([,.!?:()[\\]\\\\/\"“”\\s+])', txt)))\n",
    "\n",
    "    # remove space strings from list and convert into np array\n",
    "    tweet_split = list(filter(str.strip, temp))\n",
    "    length=len(tweet_split)\n",
    "    doc_start.extend([1]+[0]*(length-1))\n",
    "    features.extend(tweet_split)\n",
    "\n",
    "    \n",
    "ll=list(reviewsPertweet.values())\n",
    "doc_start=sum([i[0] for i in ll],[])\n",
    "features=np.array(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayesian_combination import bayesian_combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102159, 2517)"
      ]
     },
     "execution_count": 490,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data_m.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_classes = 9 # Beginning, Inside and Outside\n",
    "\n",
    "bc_model = bayesian_combination.BC(L=num_classes, K=2515, annotator_model='seq', max_iter=20,tagging_scheme='IOB2',inside_labels=[1,3,5,7], outside_label=8, beginning_labels=[0,2,4,6])\n",
    "values= bc_model.fit_predict(new_data_m.to_numpy(), np.array(doc_start), features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob=values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg=values[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_predictions=[]\n",
    "for i in tokens_count:\n",
    "    agg_predictions.append(agg[:i].tolist())\n",
    "    agg=np.delete(agg, range(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "final_agg=[list((pd.Series(agg_p)).map(num2label).values) for agg_p in agg_predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('all_agreegation', np.array(final_agg), allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 798,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_agg=np.load('all_agreegation.npy',allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 947,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_3000=pd.read_json('all_processed.jaon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 948,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_3000=list(all_3000_sen.index)\n",
    "idx_3000=[all_tweet_id.index(str(i)) for i in id_3000]\n",
    "final_3000=[final_agg[i] for i in idx_3000]\n",
    "all_3000_sen['bsc_entity_label']=final_3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 949,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_3000_sen['related_label']=all_3000_sen['related_label'].apply(ast.literal_eval)\n",
    "all_3000_sen['entity_label']=all_3000_sen['entity_label'].apply(ast.literal_eval)\n",
    "all_3000_sen['tweet_tokens']=all_3000_sen['tweet_tokens'].apply(ast.literal_eval)\n",
    "all_3000_sen['sentence_score']=all_3000_sen['sentence_score'].apply(ast.literal_eval)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 950,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mode(x): \n",
    "    try:\n",
    "        return statistics.mode(x)\n",
    "    except:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 951,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggre_revel_label(all_3000_sen):\n",
    "    all_new_record_label=[]\n",
    "    for i in range(all_3000_sen.shape[0]):\n",
    "        record=all_3000_sen.iloc[i]\n",
    "        rl_match=[]\n",
    "        for el in record['entity_label']:\n",
    "            rl_match.append([1 if ae!='O' and ce==ae else np.nan for ce,ae in zip(el,record['bsc_entity_label'])])\n",
    "        #     all_new_record_label.append(np.array(rl_match)*np.array(record['related_label']))\n",
    "        mv_r=[]\n",
    "        for x in (np.array(rl_match)*np.array(record['related_label'])).T:\n",
    "            if np.nansum(x)>0:\n",
    "                x=x[~np.isnan(x)]\n",
    "                mv_r.append(mode(x))\n",
    "            else: mv_r.append(np.nan)\n",
    "        all_new_record_label.append(np.nan_to_num(np.array(mv_r)).tolist())\n",
    "    return all_new_record_label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 952,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_3000_sen['new_related_label']=aggre_revel_label(all_3000_sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 953,
   "metadata": {},
   "outputs": [],
   "source": [
    "related_entity_label = []\n",
    "for rl, el in zip(all_3000_sen['new_related_label'], all_3000_sen['bsc_entity_label']):\n",
    "    new_el = [e if r ==1 else 'O' for r, e in zip(rl ,el)]\n",
    "    # remove 'I-xxx' not after 'B-xxx' or 'I-xxx'\n",
    "    for e, l in enumerate(new_el):\n",
    "        if l.startswith('I-') and (e == 0 or new_el[e-1][2:] != l[2:]):\n",
    "            new_el[e] = 'O'\n",
    "    related_entity_label.append(new_el)\n",
    "all_3000_sen['related_entity_label'] = related_entity_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 954,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remojize_word(s):\n",
    "    s1 = re.sub(r'EMOJI_([0-9a-zA-Z\\_]*)', r':\\1:', s)\n",
    "    return emoji.emojize(s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 955,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tokens_list, new_el_list, new_rl_list, new_rel_list = [], [], [], []\n",
    "new_zip = zip(all_3000_sen['tweet_tokens'], all_3000_sen['bsc_entity_label'], all_3000_sen['new_related_label'], all_3000_sen['related_entity_label'])\n",
    "for raw_tokens, raw_el, raw_rl, raw_rel in new_zip:\n",
    "    emoji_pos = [p for p, i in enumerate(raw_tokens) if i.startswith('EMOJI_')]\n",
    "    raw_tokens = [i if p not in emoji_pos else remojize_word(i) for p, i in enumerate(raw_tokens)]\n",
    "    new_tokens = [i for p, i in enumerate(raw_tokens) if p-1 not in emoji_pos and p+1 not in emoji_pos]\n",
    "    new_el = [i for p, i in enumerate(raw_el) if p-1 not in emoji_pos and p+1 not in emoji_pos]\n",
    "    new_rl = [i for p, i in enumerate(raw_rl) if p-1 not in emoji_pos and p+1 not in emoji_pos]\n",
    "    new_rel = [i for p, i in enumerate(raw_rel) if p-1 not in emoji_pos and p+1 not in emoji_pos]\n",
    "    new_tokens_list.append(new_tokens)\n",
    "    new_el_list.append(new_el)\n",
    "    new_rl_list.append(new_rl)\n",
    "    new_rel_list.append(new_rel)\n",
    "all_3000_sen['new_tweet_tokens'] = new_tokens_list\n",
    "all_3000_sen['n_entity_label'] = new_el_list\n",
    "all_3000_sen['n_related_label'] = new_rl_list\n",
    "all_3000_sen['n_related_entity_label'] = new_rel_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1000,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_3000_sen['sc_me_1'] = all_3000_sen['sentence_score'].apply(lambda x: [int(int(i)>=3) for i in x])\n",
    "all_3000_sen['msc_me_1'] = all_3000_sen['sc_me_1'].apply(mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 795,
   "metadata": {},
   "outputs": [],
   "source": [
    "check['entity_label_binary']=check['entity_label']\n",
    "for i in range(check.shape[0]):\n",
    "    check['entity_label_binary'].iloc[i]=[[0 if xi=='O' else 1 for xi in x] for x in check['entity_label'].iloc[i]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 868,
   "metadata": {},
   "outputs": [],
   "source": [
    "check=all_3000_sen.loc[all_3000_sen['n_related_entity_label'].apply(lambda x: [float(i!='O') for i in x])!=all_3000_sen['n_related_label']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check sentence score and relevance enttiy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1108,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ignore the relevant entity of irrelevant tweets\n",
    "def process_rl_in_irsen(re_label,sen_score):\n",
    "    if sen_score==0 and sum(re_label)>0:\n",
    "        output=[0]*len(re_label)\n",
    "    else:\n",
    "        output=re_label\n",
    "    return output\n",
    "\n",
    "\n",
    "all_aggregated_crowd['processed_n_related_label']=all_aggregated_crowd[['n_related_label','msc_me_1']].apply(lambda x: process_rl_in_irsen(x[0],x[1]),axis=1)\n",
    "\n",
    "\n",
    "\n",
    "# all_aggregated_crowd['processed_n_related_label']=all_aggregated_crowd[['n_related_label','sen_mean_t']].apply(lambda x: process_rl_in_irsen(x[0],x[1]),axis=1)\n",
    "\n",
    "\n",
    "## ignore the relevant entity of irrelevant tweets\n",
    "def process_rle_in_irsen(re_label,sen_score,ree_label):\n",
    "    if sen_score==0 and sum(re_label)>0:\n",
    "        output=['O']*len(ree_label)\n",
    "    else:\n",
    "        output=ree_label\n",
    "    return output\n",
    "\n",
    "\n",
    "all_aggregated_crowd['processed_n_related_entity_label']=all_aggregated_crowd[['n_related_label','msc_me_1','n_related_entity_label']].apply(lambda x: process_rle_in_irsen(x[0],x[1],x[2]),axis=1)\n",
    "\n",
    "# all_aggregated_crowd['processed_n_related_entity_label']=all_aggregated_crowd[['n_related_label','sen_mean_t','n_related_entity_label']].apply(lambda x: process_rle_in_irsen(x[0],x[1],x[2]),axis=1)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
