{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For process the raw data collected from Mturk\n",
    "\n",
    "input: main_batches.csv; initial_batches.csv \\\\\\\n",
    "generate: all_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import ast\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import random\n",
    "import traceback\n",
    "import html #for unescape & < >\n",
    "\n",
    "import emoji\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_3000=pd.read_csv('main_batches.csv')\n",
    "ppdict = {n: grp.loc[n].to_dict('index') for n, grp in data_3000.set_index(['InputId', 'WorkerId']).groupby(level='InputId')}\n",
    "\n",
    "new_dict={}\n",
    "for tweet_id,collection in ppdict.items():\n",
    "    basic_info=list(collection.values())[0]\n",
    "    # seperate strings but keep delimiters\n",
    "    txt = html.unescape(basic_info['Input.tweet'])\n",
    "    \n",
    "    temp = list(filter(None, re.split('([,.!?:()[\\]\"\\s+])', txt)))\n",
    "    # remove space strings from list and convert into np array\n",
    "    tweet_split = np.array(list(filter(str.strip, temp)))\n",
    "    #     tweet_split = np.array(re.split('[,.!?:()[\\]\"\\s+]', basic_info['Input.tweet']))\n",
    "#     gold_standard = basic_info['Input.prediction']\n",
    "    workerids = list(collection.keys())\n",
    "    \n",
    "    sentence_score = []\n",
    "    entity_label = []\n",
    "    related_label = []\n",
    "    workertime = []\n",
    "    assignmentID = []\n",
    "    for workerid,record in collection.items():\n",
    "        sentence_score.append(record['Answer.optradio'])\n",
    "        workertime.append(record['WorkTimeInSeconds'])\n",
    "        token_labels = np.array(['O']*len(tweet_split),dtype=np.dtype(('U',10)))\n",
    "        relation_lables = np.array([0]*len(tweet_split))\n",
    "        assignmentID.append(record['AssignmentId'])\n",
    "        try:\n",
    "            if record['Answer.related_index'] != '[]' :\n",
    "    #             print(record['Answer.related_index'])\n",
    "    #             print('undefined' in record['Answer.related_index'])\n",
    "                relation_lables_idx_str = sum([i.split(' ') for i in ast.literal_eval(record['Answer.related_index'])],[])\n",
    "\n",
    "                relation_lables_idx = list(map(int, relation_lables_idx_str))\n",
    "                relation_lables[relation_lables_idx] = 1\n",
    "\n",
    "                \n",
    "            if np.isnan(record['Answer.no-entity']) and re.split('[|]', record[\"Answer.html_output\"])[1]!='': # the value is 1 when there is no entity to label\n",
    "                html_output_list = ast.literal_eval(re.split('[|]', record[\"Answer.html_output\"])[1])\n",
    "                \n",
    "                for e in html_output_list:\n",
    "                    if 'idx' in list(e.keys()):\n",
    "\n",
    "                        if ' ' in e['idx']:\n",
    "                            idx = list(map(int, e['idx'].split(' ')))\n",
    "                        else:\n",
    "                            idx = ast.literal_eval(e['idx'])\n",
    "\n",
    "                        if type(idx) is int:\n",
    "\n",
    "#                             assert tweet_split[idx] == e['text']\n",
    "                            token_labels[idx] = 'B-'+e['className'].split('-')[1]\n",
    "                        else:\n",
    "    #                         print(' '.join(tweet_split[idx]))\n",
    "    #                         print(e['text'])\n",
    "#                             if tweet_split[idx][0] != e['text'].split()[0] and  tweet_split[idx][-1] != e['text'].split()[-1]:\n",
    "#                                 print(tweet_split[idx],e['text'])\n",
    "#                             assert tweet_split[idx][0] == e['text'].split()[0] and  tweet_split[idx][-1] == e['text'].split()[-1]\n",
    "                            idx=list(idx)\n",
    "                            token_labels[idx[0]] = 'B-'+e['className'].split('-')[1]\n",
    "                            token_labels[idx[1:]] = 'I-' + e['className'].split('-')[1]\n",
    "\n",
    "\n",
    "                \n",
    "        except Exception:\n",
    "            traceback.print_exc()\n",
    "            print('AssignmentId:',record['AssignmentId'],'Answer.related_index:',record['Answer.related_index'])\n",
    "\n",
    "        entity_label.append(token_labels.tolist())\n",
    "        related_label.append(relation_lables.tolist())\n",
    "        \n",
    "    new_dict[tweet_id]={'tweet':txt,'tweet_tokens':tweet_split.tolist(),\n",
    "                       'workerid':workerids,'workertime':workertime,'sentence_score':sentence_score,\n",
    "                        'entity_label':entity_label,'related_label':related_label,'assignmentID':assignmentID}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvfile='initial_batch.csv'\n",
    "\n",
    "data_200=pd.read_csv(csvfile)\n",
    "\n",
    "# set tweet_id+worker_id as index and group by tweet_id \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# set tweet_id+worker_id as index and group by tweet_id \n",
    "ppdict = {n: grp.loc[n].to_dict('index') for n, grp in data_200.set_index(['InputId', 'WorkerId']).groupby(level='InputId')}\n",
    "\n",
    "new_dict_i={}\n",
    "for tweet_id,collection in ppdict.items():\n",
    "    basic_info=list(collection.values())[0]\n",
    "    # seperate strings but keep delimiters\n",
    "    txt = html.unescape(basic_info['Input.tweet'])\n",
    "    temp = list(filter(None, re.split('([,.!?:()[\\]\\\\/\"“”\\s+])', txt)))\n",
    "    # remove space strings from list and convert into np array\n",
    "    tweet_split = np.array(list(filter(str.strip, temp)))\n",
    "    #     tweet_split = np.array(re.split('[,.!?:()[\\]\"\\s+]', basic_info['Input.tweet']))\n",
    "#     gold_standard = basic_info['Input.prediction']\n",
    "    workerids = list(collection.keys())\n",
    "    \n",
    "    sentence_score = []\n",
    "    entity_label = []\n",
    "    related_label = []\n",
    "    workertime = []\n",
    "    assignmentID = []\n",
    "    for workerid,record in collection.items():\n",
    "        sentence_score.append(record['Answer.optradio'])\n",
    "        workertime.append(record['WorkTimeInSeconds'])\n",
    "        token_labels = np.array(['O']*len(tweet_split),dtype=np.dtype(('U',10)))\n",
    "        relation_lables = np.array([0]*len(tweet_split))\n",
    "        assignmentID.append(record['AssignmentId'])\n",
    "        try:\n",
    "            if record['Answer.related_index'] != '[]' :\n",
    "    #             print(record['Answer.related_index'])\n",
    "    #             print('undefined' in record['Answer.related_index'])\n",
    "                relation_lables_idx_str = sum([i.split(' ') for i in ast.literal_eval(record['Answer.related_index'])],[])\n",
    "\n",
    "                relation_lables_idx = list(map(int, relation_lables_idx_str))\n",
    "                relation_lables[relation_lables_idx] = 1\n",
    "\n",
    "                \n",
    "            if np.isnan(record['Answer.no-entity']) and re.split('[|]', record[\"Answer.html_output\"])[1]!='': # the value is 1 when there is no entity to label\n",
    "                html_output_list = ast.literal_eval(re.split('[|]', record[\"Answer.html_output\"])[1])\n",
    "                \n",
    "                for e in html_output_list:\n",
    "                    if 'idx' in list(e.keys()):\n",
    "\n",
    "                        if ' ' in e['idx']:\n",
    "                            idx = list(map(int, e['idx'].split(' ')))\n",
    "                        else:\n",
    "                            idx = ast.literal_eval(e['idx'])\n",
    "\n",
    "                        if type(idx) is int:\n",
    "\n",
    "                            assert tweet_split[idx] == e['text']\n",
    "                            token_labels[idx] = 'B-'+e['className'].split('-')[1]\n",
    "                        else:\n",
    "    #                         print(' '.join(tweet_split[idx]))\n",
    "    #                         print(e['text'])\n",
    "#                             if tweet_split[idx][0] != e['text'].split()[0] and  tweet_split[idx][-1] != e['text'].split()[-1]:\n",
    "#                                 print(tweet_split[idx],e['text'])\n",
    "                            assert tweet_split[idx][0] == e['text'].split()[0] and  tweet_split[idx][-1] == e['text'].split()[-1]\n",
    "                            idx=list(idx)\n",
    "                            token_labels[idx[0]] = 'B-'+e['className'].split('-')[1]\n",
    "                            token_labels[idx[1:]] = 'I-' + e['className'].split('-')[1]\n",
    "\n",
    "\n",
    "                \n",
    "        except Exception:\n",
    "            traceback.print_exc()\n",
    "            print('AssignmentId:',record['AssignmentId'],'Answer.related_index:',record['Answer.related_index'])\n",
    "\n",
    "        entity_label.append(token_labels.tolist())\n",
    "        related_label.append(relation_lables.tolist())\n",
    "    new_dict_i[tweet_id]={'tweet':txt,'tweet_tokens':tweet_split.tolist(),\n",
    "                       'workerid':workerids,'workertime':workertime,'sentence_score':sentence_score,\n",
    "                        'entity_label':entity_label,'related_label':related_label,'assignmentID':assignmentID}\n",
    "\n",
    "\n",
    "\n",
    "d_3000=pd.DataFrame.from_dict(new_dict).T\n",
    "d_200=pd.DataFrame.from_dict(new_dict_i).T\n",
    "d_all_processed=pd.concat([d_3000,d_200]).drop_duplicates(subset='tweet')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
