{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statistics\n",
    "import warnings\n",
    "from scipy.stats import pearsonr\n",
    "warnings.filterwarnings('ignore')\n",
    "from itertools import combinations\n",
    "import random\n",
    "from seqeval.metrics import f1_score,classification_report,accuracy_score\n",
    "from sklearn.metrics import f1_score as bi_f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score as bi_accuracy_score\n",
    "from sklearn.metrics import classification_report as bi_classification_report\n",
    "import traceback\n",
    "import html #for unescape & < >\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import emoji\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "\n",
    "\n",
    "all_gs_sub=pd.read_json('/Users/ruofanhu/Desktop/submission/data/expert:gold_standard_sub/gold_standard_500.json')\n",
    "\n",
    "csvfile_500='/Users/ruofanhu/Desktop/submission/data/expert:gold_standard_sub/expert_label_500.csv'\n",
    "data_i=pd.read_csv(csvfile_500)\n",
    "\n",
    "# set tweet_id+worker_id as index and group by tweet_id \n",
    "ppdict_i = {n: grp.loc[n].to_dict('index') for n, grp in data_i.set_index(['new_tweet_id', 'WorkerId']).groupby(level='new_tweet_id')}\n",
    "\n",
    "new_dict_i={}\n",
    "for tweet_id,collection in ppdict_i.items():\n",
    "    basic_info=list(collection.values())[0]\n",
    "    # seperate strings but keep delimiters\n",
    "    txt = html.unescape(basic_info['Input.tweet'])\n",
    "    \n",
    "    temp = list(filter(None, re.split('([,.!?:()[\\]\"\\s+])', txt)))\n",
    "\n",
    "    # remove space strings from list and convert into np array\n",
    "    tweet_split = np.array(list(filter(str.strip, temp)))\n",
    "    #     tweet_split = np.array(re.split('[,.!?:()[\\]\"\\s+]', basic_info['Input.tweet']))\n",
    "#     gold_standard = basic_info['Input.prediction']\n",
    "    workerids = list(collection.keys())\n",
    "    \n",
    "    sentence_score = []\n",
    "    entity_label = []\n",
    "    related_label = []\n",
    "    workertime = []\n",
    "    assignmentID = []\n",
    "    for workerid,record in collection.items():\n",
    "        sentence_score.append(record['Answer.optradio'])\n",
    "        workertime.append(record['WorkTimeInSeconds'])\n",
    "        token_labels = np.array(['O']*len(tweet_split),dtype=np.dtype(('U',10)))\n",
    "        relation_lables = np.array([0]*len(tweet_split))\n",
    "        assignmentID.append(record['AssignmentId'])\n",
    "        try:\n",
    "            if record['Answer.related_index'] != '[]' :\n",
    "    #             print(record['Answer.related_index'])\n",
    "    #             print('undefined' in record['Answer.related_index'])\n",
    "                relation_lables_idx_str = sum([i.split(' ') for i in ast.literal_eval(record['Answer.related_index'])],[])\n",
    "\n",
    "                relation_lables_idx = list(map(int, relation_lables_idx_str))\n",
    "                relation_lables[relation_lables_idx] = 1\n",
    "\n",
    "                \n",
    "            if np.isnan(record['Answer.no-entity']) and re.split('[|]', record[\"Answer.html_output\"])[1]!='': # the value is 1 when there is no entity to label\n",
    "                html_output_list = ast.literal_eval(re.split('[|]', record[\"Answer.html_output\"])[1])\n",
    "                \n",
    "                for e in html_output_list:\n",
    "                    if 'idx' in list(e.keys()):\n",
    "\n",
    "                        if ' ' in e['idx']:\n",
    "                            idx = list(map(int, e['idx'].split(' ')))\n",
    "                        else:\n",
    "                            idx = ast.literal_eval(e['idx'])\n",
    "\n",
    "                        if type(idx) is int:\n",
    "\n",
    "                            assert tweet_split[idx] == e['text']\n",
    "                            token_labels[idx] = 'B-'+e['className'].split('-')[1]\n",
    "                        else:\n",
    "    #                         print(' '.join(tweet_split[idx]))\n",
    "    #                         print(e['text'])\n",
    "#                             if tweet_split[idx][0] != e['text'].split()[0] and  tweet_split[idx][-1] != e['text'].split()[-1]:\n",
    "#                                 print(tweet_split[idx],e['text'])\n",
    "                            assert tweet_split[idx][0] == e['text'].split()[0] and  tweet_split[idx][-1] == e['text'].split()[-1]\n",
    "                            idx=list(idx)\n",
    "                            token_labels[idx[0]] = 'B-'+e['className'].split('-')[1]\n",
    "                            token_labels[idx[1:]] = 'I-' + e['className'].split('-')[1]\n",
    "\n",
    "\n",
    "                \n",
    "        except Exception:\n",
    "            traceback.print_exc()\n",
    "            print('AssignmentId:',record['AssignmentId'],'Answer.related_index:',record['Answer.related_index'])\n",
    "\n",
    "        entity_label.append(token_labels.tolist())\n",
    "        related_label.append(relation_lables.tolist())\n",
    "    new_dict_i[tweet_id]={'tweet':txt,'tweet_tokens':tweet_split.tolist(),\n",
    "                       'workerid':workerids,'workertime':workertime,'sentence_score':sentence_score,\n",
    "                        'entity_label':entity_label,'related_label':related_label,'assignmentID':assignmentID}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "g=pd.DataFrame.from_dict(new_dict_i)\n",
    "gg=g.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gg['sentence_class']=gg['sentence_score'].apply(lambda x: int(x[0]>3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_gs_sub=all_gs_sub.sort_values(by ='new_tweet_id' )\n",
    "new_gg=gg.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.8189806678383128 \n",
      "ACC: 0.794 \n",
      "AUC: 0.802472897555969\n"
     ]
    }
   ],
   "source": [
    "\n",
    "gt_sc=new_gg['sentence_class'].tolist()\n",
    "crowd_sc=all_gs_sub['sentence_class'].tolist()\n",
    "# crowd_sc=all_cd_expert_labeled_data['sen_mean_t'].tolist()\n",
    "\n",
    "\n",
    "print('F1:',bi_f1_score(gt_sc,crowd_sc),\n",
    "      '\\nACC:',bi_accuracy_score(gt_sc,crowd_sc),\n",
    "      '\\nAUC:',roc_auc_score(gt_sc,crowd_sc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create mat for krippendorff's alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('expert_label_500.csv')\n",
    "\n",
    "\n",
    "dj=data.to_json(orient=\"records\")\n",
    "parsed = json.loads(dj)\n",
    "\n",
    "def process_entity_label(record):\n",
    "    txt = html.unescape(record['Input.tweet'])\n",
    "    \n",
    "    temp = list(filter(None, re.split('([,.!?:()[\\]\"\\s+])', txt)))\n",
    "\n",
    "    # remove space strings from list and convert into np array\n",
    "    tweet_split = np.array(list(filter(str.strip, temp)))\n",
    "\n",
    "\n",
    "    token_labels = np.array(['']*len(tweet_split),dtype=np.dtype(('U',10)))\n",
    "#     if record['Answer.related_index'] != '[]' :\n",
    "#         relation_lables_idx_str = sum([i.split(' ') for i in ast.literal_eval(record['Answer.related_index'])],[])\n",
    "\n",
    "#         relation_lables_idx = list(map(int, relation_lables_idx_str))\n",
    "#         relation_lables[relation_lables_idx] = 1\n",
    "\n",
    "                \n",
    "    if record['Answer.no-entity'] is None and re.split('[|]', record[\"Answer.html_output\"])[1]!='': # the value is 1 when there is no entity to label\n",
    "        html_output_list = ast.literal_eval(re.split('[|]', record[\"Answer.html_output\"])[1])\n",
    "                \n",
    "        for e in html_output_list:\n",
    "            if 'idx' in list(e.keys()):\n",
    "\n",
    "                if ' ' in e['idx']:\n",
    "                    idx = list(map(int, e['idx'].split(' ')))\n",
    "                else:\n",
    "                    idx = ast.literal_eval(e['idx'])\n",
    "\n",
    "                if type(idx) is int:\n",
    "\n",
    "#                             assert tweet_split[idx] == e['text']\n",
    "                    token_labels[idx] = 'B-'+e['className'].split('-')[1]\n",
    "                else:\n",
    "                    idx=list(idx)\n",
    "                    token_labels[idx[0]] = 'B-'+e['className'].split('-')[1]\n",
    "                    token_labels[idx[1:]] = 'I-' + e['className'].split('-')[1]\n",
    "\n",
    "\n",
    "\n",
    "    return token_labels.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewsPerUser = defaultdict(list)\n",
    "reviewsPerItem = defaultdict(list)\n",
    "reviewsPertweet = defaultdict(list)\n",
    "for d in parsed:\n",
    "    entity_labels=process_entity_label(d)\n",
    "\n",
    "\n",
    "    for i in range(len(entity_labels)):\n",
    "        user,item = 'expert', str(d['new_tweet_id'])+','+str(i)\n",
    "        \n",
    "        reviewsPerItem[item].append({'WorkerId':user,'entity_type':entity_labels[i]})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add gold_standard label\n",
    "input_id=all_gs_sub.index\n",
    "entity_labels_l=all_gs_sub['entity_label'].tolist()\n",
    "for i in range(all_gs_sub.shape[0]):\n",
    "    entity_labels=entity_labels_l[i]\n",
    "    \n",
    "    \n",
    "    for j in range(len(entity_labels)):\n",
    "        user,item = 'gold_standard', str(input_id[i])+','+str(j)\n",
    "\n",
    "        reviewsPerItem[item].append({'WorkerId':user,'entity_type':entity_labels[j]})\n",
    "\n",
    "lu=2\n",
    "li = len(reviewsPerItem)\n",
    "# data_ = np.empty((li, lu))\n",
    "# data_[:] = np.nan\n",
    "# data__t = np.empty((lu, li))\n",
    "ku = ['expert','gold_standard']\n",
    "ki = list(reviewsPerItem.keys())\n",
    "\n",
    "# for i in range(li):\n",
    "#     for r in reviewsPerItem[ki[i]]:\n",
    "#         data_[i][ku.index(r['WorkerId'])] =r['entity_type']\n",
    "\n",
    "# Construct the P Matrix\n",
    "data_m = pd.DataFrame('',columns=ku,index=ki)\n",
    "for i in range(li):\n",
    "    for r in reviewsPerItem[ki[i]]:\n",
    "        if r['entity_type']!='O' and r['entity_type']!='' :\n",
    "            \n",
    "            data_m.loc[ki[i]][ku.index(r['WorkerId'])] = r['entity_type'].split('-')[1]\n",
    "        \n",
    "        else:\n",
    "        \n",
    "            data_m.loc[ki[i]][ku.index(r['WorkerId'])] = r['entity_type']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data_m=data_m.replace('O','')\n",
    "new_data_m.to_csv('input_to_software.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
