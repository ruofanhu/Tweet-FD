[config]
path_data = /work/dzhang5/usda_project/benchmark/tweet-fd/tweet_fd.p
label_map = /work/dzhang5/usda_project/benchmark/tweet-fd/label_map.json
path_train = /work/dzhang5/usda_project/benchmark/IMGJM-master/dataset/tweet-df/x_train.txt
path_train_y = /work/dzhang5/usda_project/benchmark/IMGJM-master/dataset/tweet-df/y_train.npy
path_dev = /work/dzhang5/usda_project/benchmark/IMGJM-master/dataset/tweet-df/x_dev.txt
path_dev_y = /work/dzhang5/usda_project/benchmark/IMGJM-master/dataset/tweet-df/y_dev.npy
path_test = /work/dzhang5/usda_project/benchmark/IMGJM-master/dataset/tweet-df/x_test.txt
path_test_y = /work/dzhang5/usda_project/benchmark/IMGJM-master/dataset/tweet-df/y_test.npy
filename_tags = /work/dzhang5/usda_project/benchmark/tweet-fd/tags.txt
task_type= relevant_entity_detection
default_label = O
ntags = 9
model_selector = S_valid_F:high
preload_vectors = /work/dzhang5/usda_project/benchmark/IMGJM-master/embeddings/glove.840B.300d.txt
word_embedding_size = 300
emb_initial_zero = False
train_embeddings = True
char_embedding_size = 100
word_recurrent_size = 300
char_recurrent_size = 100
hidden_layer_size = 50
char_hidden_layer_size = 50
lowercase = True
replace_digits = True
min_word_freq = -1
singletons_prob = 0.1
allowed_word_length = -1
max_train_sent_length = 55
vocab_include_devtest = True
vocab_only_embedded = False
initializer = glorot
opt_strategy = adam
learningrate = 0.001
clip = 0.0
batch_equal_size = False
max_batch_size = 32
epochs = 20
stop_if_no_improvement_for_epochs = -1
learningrate_decay = 0.9
dropout_input = 0.5
dropout_word_lstm = 0.5
dropout_label_lstm = 0.5
tf_per_process_gpu_memory_fraction = 1.0
tf_allow_growth = True
lmcost_max_vocab_size = 7500
lmcost_hidden_layer_size = 50
lmcost_lstm_gamma = 0.0
lmcost_joint_lstm_gamma = 0.0
lmcost_char_gamma = 0.0
lmcost_joint_char_gamma = 0.0
char_integration_method = concat
save = saved_model_multi_re_mgade
load =
garbage_collection = False
lstm_use_peepholes = False
whidden_layer_size = 200
attention_evidence_size = 100
attention_activation = multi
attention_objective_weight = 0.0
sentence_objective_weight = 1.0
sentence_objective_persistent = True
word_objective_weight = 1.0
sentence_composition = attention
random_seed = 100
word_loss_function = crossentropy
sentence_loss_function = squareddiff
masked_attention_weights = True
override_for_aro = False
un_masked_fields_for_attention_weights = {"O": 0, "B-food": 1, "I-food": 2, "B-symptom": 3, "I-symptom": 4, "B-loc": 5, "I-loc": 6, "B-other": 7, "I-other": 8}
double_attention_selective_label_weight = True
double_attention_predicted_label_weight = True
mask0_predicted_label_weight = False
double_attention_predicted_label_weight_jointype = concat
exclude_0_for_aro = False
double_attention_predicted_label = False
mask_attention_from_label = False
masked_fields_attention_from_label = None
double_attention_unsupervised_self_attention = False
double_attention_unsupervised_self_attention_jointype = concat
performance_file = /work/dzhang5/usda_project/benchmark/all_test_performance.txt